{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Imports\n",
    "This is going to be a more granular, customizable neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "import pathlib as path\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read-in\n",
    "unchanged from neuralnet.ipynb; this is non-negotiable. \n",
    "For any of the user changes to work, I need to know the shape of the data; so this split will be universal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenda\\AppData\\Local\\Temp\\ipykernel_27940\\3039068683.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop(index=19999, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "rawData = pd.read_csv('STEMVisualsSynthData.csv', header=0)\n",
    "#remove unneeded column\n",
    "rawData.drop('Index_within_Experiment', axis = 1, inplace = True)\n",
    "#X is inputs--the three Concentrations, F_in, I0 (light intensity), and c_N_in (6)\n",
    "X = rawData[['Time', 'C_X', 'C_N', 'C_L', 'F_in', 'C_N_in', 'I0']]\n",
    "Y = X.copy(deep=True)\n",
    "#drop unnecessary rows in Y\n",
    "Y.drop('F_in', axis = 1, inplace = True)\n",
    "Y.drop('C_N_in', axis = 1, inplace = True)\n",
    "Y.drop('I0', axis = 1, inplace = True)\n",
    "Y.drop('Time', axis = 1, inplace = True)\n",
    "#Y vals should be X concentrations one timestep ahead, so remove the first index\n",
    "Y.drop(index=0, inplace=True)\n",
    "#To keep the two consistent, remove the last index of X\n",
    "X.drop(index=19999, inplace=True)\n",
    "#separate the times out into their own little thing for later use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "Keeping it simple stupid, there's only 1 split happening here. User chooses the percentage of data to hold out for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaults\n",
    "train_ratio = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1 - train_ratio)\n",
    "\n",
    "#function to call whenever wanting to change the train/test split\n",
    "def trainSplit():\n",
    "    train_ratio = float(input(\"What percentage of data to holdout for testing? (default 0.2) \"))\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1 - train_ratio)\n",
    "\n",
    "#Separate Time out--we don't want this as a feature the model learns (since it's already incorporated in by how X and Y are structured!)\n",
    "#however, having the time values will be useful for plotting later\n",
    "XTrainTime = X_train.pop('Time')\n",
    "XTestTime = X_test.pop('Time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Another thing I'm going to just have to have happen backend without the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for both x and y, create a min max scaler\n",
    "mmscalerX = preprocessing.MinMaxScaler()\n",
    "#we'll fit and transform based on training data\n",
    "X_train_minmax = mmscalerX.fit_transform(X_train)\n",
    "#transform testing data based on training data\n",
    "X_test_minmax = mmscalerX.transform(X_test)\n",
    "#same process as above for Y\n",
    "mmscalerY = preprocessing.MinMaxScaler()\n",
    "Y_train_minmax = mmscalerY.fit_transform(Y_train)\n",
    "Y_test_minmax = mmscalerY.transform(Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put our data into tensors\n",
    "X_train = torch.tensor(X_train_minmax, dtype=torch.float32)\n",
    "y_train = torch.tensor(Y_train_minmax, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_minmax, dtype=torch.float32)\n",
    "y_test = torch.tensor(Y_test_minmax, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "Here's where the majority of new stuff comes in--we need to let the user have control over a lot of things.\n",
    "- Num Layers: How many patterns of Linear/activation to use? range: 1-5\n",
    "- Num Nodes: How many hidden nodes per layer? range: 5-20\n",
    "- Optimizer: How does the model update its weights? choices: ADAM, SGD\n",
    "- Loss: How does the model measure accuracy? choices: L1, MSE, LKDivLoss\n",
    "- Learning Rate: How greatly can the optimizer change weights? range: 0.00001-0.1\n",
    "- Epochs: How many times to run the training data through the model? range: 10-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLayers = int(input(\"How many layers? (default 2, range 1-5) \"))\n",
    "numNodes = int(input(\"How many nodes per layer? (default 10, range 5-20) \"))\n",
    "#set up the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], numNodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(numNodes, numNodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(numNodes, numNodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(numNodes, y_train.shape[1]),\n",
    ")\n",
    "lossChoice = int(input(\"Which loss function? (1: L1, 2: MSE, 3: KLDiv) \"))\n",
    "#set up the loss function\n",
    "match lossChoice:\n",
    "    case 1:\n",
    "        loss_fn = nn.L1Loss()\n",
    "    case 2:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    case 3:\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    case _:\n",
    "        print(\"Invalid loss function choice. Defaulting to L1.\")\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "#set up the optimizer\n",
    "optimChoice = int(input(\"Which optimizer? (1: SGD, 2: Adam) \"))\n",
    "lr = float(input(\"Learning rate? (default 0.01) \"))\n",
    "match optimChoice:\n",
    "    case 1:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    case 2:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    case _:\n",
    "        print(\"Invalid optimizer choice. Defaulting to SGD.\")\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Mostly unchanged. User can change:\n",
    "- num of Epochs, range: 10-1000\n",
    "- Batch size, range: 100-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training parameters\n",
    "n_epochs = int(input(\"How many epochs? (default 100) \"))\n",
    "batch_size = int(input(\"Batch size? (default 100) \"))\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    " \n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Graph\n",
    "(unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
